{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#built in librairies \n",
    "from tqdm import tqdm\n",
    "\n",
    "# pip libraires\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# visualization librairies\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Natural Language Processing Librairies\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Visualization librairies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "#Calculating libraries\n",
    "import scipy as sp\n",
    "\n",
    "#statistical librairies\n",
    "import pingouin \n",
    "\n",
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "\n",
    "#from genres import genre_wordsets\n",
    "import csv\n",
    "import datetime  \n",
    "from dateutil.relativedelta import relativedelta\n",
    "from utils.genres import additional_wordsets\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"read table tvtropes and process the data to get rid of json format\n",
    "\"\"\"\n",
    "summaries=pd.read_csv('./data/plot_summaries.txt',sep=\"\\t\",header=None)\n",
    "summaries.rename(columns={0:'wikipedia_movie_id',1:'Plot_summaries'},inplace=True)\n",
    "movies=pd.read_table('./data/movie.metadata.tsv',header=None)\n",
    "movies.rename(columns={0:'wikipedia_movie_id',1:'rebase_movie_ID',2:'Movie_name',3:'Movie_release',4:'Box_office_revenue',5:'Movie_runtime',6:'Movie_language',7:'Movie_country',8:'Movie_genre'},inplace=True)\n",
    "\n",
    "df_summaries = pd.merge(summaries, movies[['wikipedia_movie_id', 'Movie_name']], on='wikipedia_movie_id')\n",
    "pd.set_option('max_colwidth', 200)\n",
    "df_summaries.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import spacy_transformers\n",
    "\n",
    "# Others\n",
    "from functools import partial\n",
    "#Vader\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "nlp_fast =spacy.load('en_core_web_sm')\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')# en_core_web_sm, en_core_web_trf, xx_ent_wiki_sm\n",
    "\n",
    "doc = nlp(\" Marcus roams through the desert with Grubert where Vietnamese terrorist are roaming in Hgsvssd. in 1945 Vietmam was at war with the US as well as the Vietnames\")# Display Entities\n",
    "doc = nlp(df_summaries.iloc[7]['Plot_summaries'])\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(x:str)->list:\n",
    "    \"\"\"Â Returns the lemmatized of every word in the input\n",
    "\n",
    "    Args:\n",
    "        x (str): Contains the input to be lemmatized\n",
    "\n",
    "    Returns:\n",
    "        list: a list of the words lemmatized\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a stemmer, the Lancaster Stemmer is used here\n",
    "    doc = nlp_fast(x)\n",
    "    # Return a list of Stemmed words\n",
    "    return \" \".join([token.lemma_.capitalize() for token in doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd \n",
    "import geopy \n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import requests\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "\n",
    "user_agent = 'user_me_{}'.format(randint(10000,99999))\n",
    "locator = Nominatim(user_agent=user_agent)\n",
    "\n",
    "geocode = RateLimiter(locator.geocode, min_delay_seconds=1.2)\n",
    "\n",
    "def find_country(country):\n",
    "    address = geocode(country)\n",
    "    if address is None:\n",
    "        return \"Other\"\n",
    "    lat = address.raw['lat']\n",
    "    lon = address.raw['lon']\n",
    "    url = f'https://nominatim.openstreetmap.org/reverse?lat={lat}&lon={lon}&format=json&accept-language=en&zoom=3'\n",
    "    \n",
    "    try:\n",
    "        result = requests.get(url=url)\n",
    "        result_json = result.json()\n",
    "        return result_json['display_name']\n",
    "    except Exception as e:\n",
    "        return \"Other\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_mean(dict_list):\n",
    "    mean_dict = {}\n",
    "    for key in dict_list[0].keys():\n",
    "        mean_dict[key] = sum(d[key] for d in dict_list) / len(dict_list)\n",
    "    return mean_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_val = len(df_summaries)\n",
    "import collections, functools, operator\n",
    "\n",
    "def find_movie_location_and_sentiment(summary):\n",
    "    where = {}\n",
    "    who = {}\n",
    "    countries = {}\n",
    "    \n",
    "    doc = nlp(summary)\n",
    "    vs_all = analyzer.polarity_scores(summary)\n",
    "\n",
    "    for sent in doc.sents :\n",
    "        \n",
    "        phrase_sentiment = analyzer.polarity_scores(str(sent))\n",
    "\n",
    "        for ent in sent.ents :\n",
    "            if ent.label_ in ['GPE']:\n",
    "                country = find_country(clean_string(ent.text))\n",
    "\n",
    "                if clean_string(ent.text) in where:\n",
    "                    where[clean_string(ent.text)].append(phrase_sentiment)\n",
    "                else:\n",
    "                    where[clean_string(ent.text)] = list([phrase_sentiment])\n",
    "\n",
    "                if country in countries:\n",
    "                    countries[country].append(phrase_sentiment)\n",
    "                else:\n",
    "                    countries[country] = list([phrase_sentiment])\n",
    "\n",
    "            if ent.label_ in ['NORP']:\n",
    "                if clean_string(ent.text) in who:\n",
    "                    who[clean_string(ent.text)].append(phrase_sentiment)\n",
    "                else:\n",
    "                    who[clean_string(ent.text)] = list([phrase_sentiment])\n",
    "\n",
    "    for key,value in where.items():\n",
    "        dict_mean(value)\n",
    "        where[key] = dict_mean(value)\n",
    "    \n",
    "    for key,value in who.items():\n",
    "        who[key] = dict_mean(value)\n",
    "\n",
    "    for key,value in countries.items():\n",
    "        countries[key] = dict_mean(value)\n",
    "\n",
    "    return pd.Series([who, where, countries, vs_all])\n",
    " \n",
    "# function to apply \n",
    "df_summaries[['who', 'where', 'country', 'sentiment']] = df_summaries[6:8].Plot_summaries.apply(lambda x: find_movie_location_and_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "maxval = len(df_summaries)\n",
    "step = 1000\n",
    "i = 0\n",
    "iteration = i*step\n",
    "while i*step +step < maxval:\n",
    "    print(\"Step \", i)\n",
    "    user_agent = 'user_me_{}'.format(randint(10000,99999))\n",
    "    locator = Nominatim(user_agent=user_agent)\n",
    "    geocode = RateLimiter(locator.geocode, min_delay_seconds=1)\n",
    "\n",
    "    df_summaries_trunc = df_summaries[i*step:i*step + step]\n",
    "    df_summaries_trunc[['who', 'where', 'country', 'sentiment']] = df_summaries_trunc.Plot_summaries.apply(lambda x: find_movie_location_and_sentiment(x))\n",
    "    df_summaries_trunc.to_csv('tmp_stitches/country_sentiment_stitch_'+ str(i) + '.csv', index=True)\n",
    "    i+=1\n",
    "\n",
    "df_summaries_trunc = df_summaries[i*step:i*step + 204]\n",
    "df_summaries_trunc[['who', 'where', 'country', 'sentiment']] = df_summaries_trunc.Plot_summaries.apply(lambda x: find_movie_location_and_sentiment(x))\n",
    "df_summaries_trunc.to_csv('tmp_stitches/country_sentiment_stitch_'+ str(i) + '.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42204\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ast import literal_eval\n",
    "\n",
    "final_df = pd.read_csv('tmp_stitches/country_sentiment_stitch_'+ str(0) + '.csv',header=0, index_col=0)\n",
    "\n",
    "for i in range(1,43):\n",
    "    tmp_df = pd.read_csv('tmp_stitches/country_sentiment_stitch_'+ str(i) + '.csv',header=0, index_col=0)\n",
    "    final_df = pd.concat([final_df, tmp_df], ignore_index=True)\n",
    "\n",
    "print(len(final_df))\n",
    "final_df[['who','where','country','sentiment']] = final_df[['who','where','country','sentiment']].applymap(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('tmp_stitches/country_sentiment_stitch.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eff2cc948ef6ebc3a0ca17a8e91ae3fd1666d3f54a95771a7ecbf9e32095cb44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
